{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MagicBricks scraper script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will contain the code related to scraping the Magic Bricks website (www.magicbricks.com). There will be 3 kinds of urls. BASE URL will be the first URL to open the page with all the properties for sale. SERVICE URL will be the service call to extract the properties in chunks of 30 and the SERVICE PAYLOAD will contain the parameters related to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from numpy import NaN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Both are GET requests to be fired\n",
    "BASE_URL = 'https://www.magicbricks.com/property-for-sale/residential-real-estate?proptype=Multistorey-Apartment,Builder-Floor-Apartment,Penthouse,Studio-Apartment&cityName=Bangalore'\n",
    "# service url will be the url to collect the individual house related data.\n",
    "SERVICE_URL = 'https://www.magicbricks.com/mbsearch/propertySearch.html' \n",
    "# Payload will carry the additional parameters required for the service url\n",
    "SERVICE_PAYLOAD = {\n",
    "        'propertyType_new' : '10002_10003_10021_10022',\n",
    "        'city':3327,\n",
    "        'searchType':1,\n",
    "        'propertyType':'10002,10003,10021,10022',\n",
    "        'disWeb':'',\n",
    "        'pType':'10002,10003,10021,10022',\n",
    "        'category':'S',\n",
    "        'groupstart':30,\n",
    "        'offset':'',\n",
    "        'maxOffset':'',\n",
    "        'attractiveIds':'',\n",
    "        'page':2,\n",
    "        'ltrIds':'',\n",
    "        'preCompiledProp':'',\n",
    "        'excludePropIds':'',\n",
    "        'addpropertyDataSet':''\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions to cater the needs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will extract the URL from the extracted string.\n",
    "def extractUrl(st):\n",
    "    l = re.findall(r\"'(.*?)'\",st)[0].replace('\\\\','')\n",
    "    _id = l.split('&')[-1].split('=')[-1]\n",
    "    return [l,_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by hitting the base URL and extract the required parameters and fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hitting the base url with get request\n",
    "req = requests.get(BASE_URL)\n",
    "\n",
    "# Converting the request response content to the soup parser for easier extraction and use\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "# Pattern to identify the individual property URL\n",
    "pat = r'(if\\(openDetailPage\\(event, .*?\\)\\))'\n",
    "\n",
    "#st = str(req.content)\n",
    "\n",
    "# Extracting all matches for the pattern\n",
    "li = re.findall(pat, str(req.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the parameters that will be required for service URLs which will be fired later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ltrIds = soup.find('span',{'id':'ltrIds'}).text\n",
    "scrText = soup.find('div',{'id':'resultDiv'}).find('script').text\n",
    "\n",
    "scrText = scrText.replace('var gsData = ','').replace('\\r\\n','').replace('\\t','').replace(':','\\':').replace('{','{\\'',1).replace(',',',\\'').replace('\\'','\"').replace(',\"}','}')\n",
    "# scrText = scrText\n",
    "# scrText = scrText\n",
    "# scrText = scrText\n",
    "# scrText = scrText\n",
    "# scrText = scrText\n",
    "# scrText = scrText\n",
    "# scrText = scrText\n",
    "\n",
    "service_params = json.loads(scrText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to extract the URLs from the strings and save them into a file in order to prevent loss of data at later steps due to any error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "property_URLs = [extractUrl(x) for x in li]\n",
    "# filename = 'URLs_1'\n",
    "# ext = '.txt'\n",
    "# with open(filename+ext,'w') as f:\n",
    "#     for u in property_URLs:\n",
    "#         f.write(u[0]+';'+u[1]+'\\n')\n",
    "#     f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make the file saving part into a method with a slight change as to append for every time urls are extracted from the service url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def appendURLsToFile(properties,filename):\n",
    "    with open(filename,'a') as f:\n",
    "        for u in properties:\n",
    "            f.write(u[0]+';'+u[1]+'\\n')\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will get the properties with the help of service URL. The same process till now has to be followed just that it will be with the service URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_pages = int(service_params['pageCount'])\n",
    "SERVICE_PAYLOAD['maxOffset'] = service_params['maxOffset']\n",
    "SERVICE_PAYLOAD['offset'] = service_params['offset']\n",
    "SERVICE_PAYLOAD['ltrIds'] = ltrIds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Payload will be carrying the additional parameters required for the service url. Only some of them will have to be updated everytime which are offset, maxOffset, page, groupStart.\n",
    "\n",
    "The function below will extract property urls with the help of service url "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractProperties(filename):\n",
    "    req = requests.get(SERVICE_URL,params=SERVICE_PAYLOAD)\n",
    "    esoup = BeautifulSoup(req.content,'html.parser')\n",
    "    matches = re.findall(pat,str(req.content))\n",
    "    \n",
    "    properties = [extractUrl(x) for x in matches]\n",
    "    appendURLsToFile(properties,filename)\n",
    "    \n",
    "    sc = esoup.find('script').text\n",
    "    t = sc.replace(' ','').replace('\\n\\t','').replace('\\'','').split(';')[:-1]\n",
    "    SERVICE_PAYLOAD['groupstart'] = int(SERVICE_PAYLOAD['groupstart']) + 30\n",
    "    #SERVICE_PAYLOAD['offset'] = t[1].split('=')[1]\n",
    "    #SERVICE_PAYLOAD['maxOffset'] = t[2].split('=')[1]\n",
    "    SERVICE_PAYLOAD['page'] = int(SERVICE_PAYLOAD['page']) + 1\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The service request will be fired from initial count 2 to the max_pages available. The parameters offset and maxoffset don't seem to have much effect on the results. So, we are setting them to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVICE_PAYLOAD['offset'] = 0\n",
    "SERVICE_PAYLOAD['maxOffset'] = 0\n",
    "\n",
    "it = 2\n",
    "while it < max_pages:\n",
    "    file_name = 'URLs_' + str(int(it/50)+1)\n",
    "    extractProperties(file_name+ext)\n",
    "    print(str(it) + ' pages URLs extracted')\n",
    "    it += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Till here we have extracted the URLs and IDs of properties available for sale on the platform. Let's concat all the files and create a master file storing all URLs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('URLs_*.txt')\n",
    "with open('URLsMaster.txt','a') as m:\n",
    "    for f in files:\n",
    "        with open(f,'r') as e:\n",
    "            m.writelines(e.readlines())\n",
    "            e.close()\n",
    "    m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the master file with all the URLs is created. From here we will extract the data for individual property\n",
    "\n",
    "Now we have the data to be collected from the webpage. As there are higher number of URLs to hit and get records from, we will try to achieve this by going file by file. Each file with URLs can be taken for an iteration and then property details can be loaded into a dataframe. After capturing the parameters from the file, we will save the dataframe into a csv file for later reference.\n",
    "\n",
    "First we will perform a sequence of steps before hitting the URLs. \n",
    "1. Take the list of files to be used.\n",
    "2. Create the steps for iteration\n",
    "3. For iteration:\n",
    "        a. Create a dataframe.\n",
    "        b. Load the URLs from file.\n",
    "        c. For each URL:\n",
    "            i. Create a column for IDs from the URL. \n",
    "            ii. Load the header properties and format them.\n",
    "            iii. Load the data attributes from the domCache_detailpage element.\n",
    "            iv. Load these into the dataframe. \n",
    "        d. After the loop is done, save the dataframe into a file with the same name as the URL file but with an extension '.csv'\n",
    "4. Make sure the loop wouldn't stop due to exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "files = glob.glob('URLs_*.txt')\n",
    "\n",
    "def forEachURL(url):\n",
    "    splits = url.split(';')\n",
    "    urlstring = splits[0]\n",
    "    _id = splits[1]\n",
    "    req = requests.get(urlstring)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "    infos = soup.findAll('div',{'class':'p_infoColumn'})\n",
    "    details = {}\n",
    "    for ta in infos:\n",
    "        t = ta.find('div',{'class':'p_title'}).text.replace('\\n','')\n",
    "        v = ta.find('div',{'class':'p_value'}).text.replace('\\n','')\n",
    "        details[t] = v\n",
    "    data_attrs = soup.find('span',{'id':'domcache_detailpage'})\n",
    "    domcache = {}\n",
    "    try:\n",
    "        domcache = data_attrs.attrs\n",
    "    except AttributeError:\n",
    "        domcache = {}\n",
    "    #print('Data obtained for URL with ID: ' + _id)\n",
    "    return { 'propid': _id, **details, **domcache}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def forFile(filename):\n",
    "    df_cur = pd.DataFrame()\n",
    "    lines = [line.rstrip('\\n') for line in open(filename)]\n",
    "    props = [forEachURL(url) for url in lines]\n",
    "    df_cur = pd.DataFrame(props)\n",
    "    df_cur.to_csv(filename.replace('.txt','.csv'))\n",
    "    print('File with data created: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in files[1:]:\n",
    "    forFile(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob.glob('URLs_*.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a master data file containing all the data for every URL extracted. This will be our master data store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = pd.DataFrame()\n",
    "for f in data_files:\n",
    "    df_cur = pd.read_csv(f)\n",
    "    df_master = df_master.append(df_cur, sort=False)\n",
    "    \n",
    "df_master.to_csv('URLsDataMaster_1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all the data that can be collected from the Magic Bricks website. The next part is the ratings for the locality that have to be collected. After initial exploration of the traffic generated for the web pages, we get that the ratings for the different localities are fetched from a different source(https://rating.magicbricks.com/mbRating/getWiget.json). So, we will again fire different set of requests for fetching the ratings of the localities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locs = pd.DataFrame(df_master['data-localityid'].unique())\n",
    "df_locs.to_csv('LocalityIds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locsload = pd.read_csv('LocalityIds.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_locsload.columns = ['slno','LocID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractRating(locId):\n",
    "    key = '5e8e646d535182a72b693a4669ff9e'\n",
    "    url = 'https://rating.magicbricks.com/mbRating/getWiget.json'\n",
    "    params = {\n",
    "        'callback':'newDetailsRatingAndReviewWidget',\n",
    "        'host':'magicbricks.com',\n",
    "        'key':key,\n",
    "        'code':locId,\n",
    "        'type':'LOCALITY'\n",
    "    }\n",
    "    resp = requests.get(url,params=params)\n",
    "    resp = resp.text.replace(');','').replace('newDetailsRatingAndReviewWidget(','')\n",
    "    j = json.loads(resp)\n",
    "    rat = j['avgRating']\n",
    "    plcs = {}\n",
    "    try:\n",
    "        plcs = j['categoryRatingMap']['Places of Interest']\n",
    "    except KeyError:\n",
    "        plcs = {}\n",
    "    return { 'rating' : rat, **plcs, 'locId' : locId}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dics = [extractRating(int(s)) for s in df_locsload['LocID']]\n",
    "df_locdata = pd.DataFrame(dics)\n",
    "df_locdata.to_csv('locs_data1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have the required data from the magicbricks.com domain. There are requirements of some other location based data like geospatial data of the locations of the properties but the sources are yet to be found. We will proceed with the data collected so far and create our model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
